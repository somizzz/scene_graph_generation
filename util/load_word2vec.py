import os
import sys
import torch
import numpy as np
from gensim.models import KeyedVectors
from tqdm import tqdm
import requests 
import gzip
import shutil

import gensim.downloader as api

# è§£å‹ .bin.gz æ–‡ä»¶ï¼ˆå¦‚æœå°šæœªè§£å‹ï¼‰

bin_path = "/home/p_zhuzy/p_zhu/PySGG-main/datasets/vg/stanford_spilt/word2vec/GoogleNews-vectors-negative300.bin"


# åŠ è½½æ¨¡å‹
try:
    word2vec_model = KeyedVectors.load_word2vec_format(bin_path, binary=True)
    print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print("è¯æ±‡è¡¨å¤§å°:", len(word2vec_model.key_to_index))
except Exception as e:
    print(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")



def load_word_vectors(root, wv_type, dim):
    """
    åŠ è½½ Word2Vec è¯å‘é‡å¹¶è½¬æ¢ä¸º PyTorch æ ¼å¼ã€‚
    :param root: è¯å‘é‡æ–‡ä»¶çš„å­˜å‚¨ç›®å½•ã€‚
    :param wv_type: è¯å‘é‡æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ã€‚
    :param dim: è¯å‘é‡çš„ç»´åº¦ã€‚
    :return: (wv_dict, wv_arr, wv_size)
        - wv_dict: å•è¯åˆ°ç´¢å¼•çš„å­—å…¸ã€‚
        - wv_arr: è¯å‘é‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º [è¯æ±‡è¡¨å¤§å°, dim]ã€‚
        - wv_size: è¯å‘é‡çš„ç»´åº¦ã€‚
    """
    if isinstance(dim, int):
        dim = str(dim) + 'd'

    # æ–‡ä»¶è·¯å¾„
    fname_bin = os.path.join(root, wv_type + '.bin')
    fname_txt = os.path.join(root, wv_type + '.txt')
    fname_pt = os.path.join(root, wv_type + '.' + dim + '.pt')

    # å¦‚æœæœ‰ .pt ç›´æ¥åŠ è½½ï¼Œé¿å…é‡å¤è§£æ .bin
    if os.path.isfile(fname_pt):
        print(f"ğŸ”¹ ç›´æ¥åŠ è½½ PyTorch è¯å‘é‡: {fname_pt}")
        try:
            return torch.load(fname_pt, map_location=torch.device("cpu"))
        except Exception as e:
            raise RuntimeError(f"âš ï¸ åŠ è½½ .pt å¤±è´¥: {e}")

    # å¦‚æœæ²¡æœ‰ .ptï¼Œå°±åŠ è½½ .bin æˆ– .txt
    if os.path.isfile(fname_bin):
        print(f"ğŸ”¹ åŠ è½½ Word2Vec .bin æ–‡ä»¶: {fname_bin}")
        binary_format = True
        fname = fname_bin
    elif os.path.isfile(fname_txt):
        print(f"ğŸ”¹ åŠ è½½ Word2Vec .txt æ–‡ä»¶: {fname_txt}")
        binary_format = False
        fname = fname_txt
    else:
        raise RuntimeError(f"æ‰¾ä¸åˆ° {fname_bin} æˆ– {fname_txt}")

    try:
        # åŠ è½½ Word2Vec æ¨¡å‹
        wv_model = KeyedVectors.load_word2vec_format(fname, binary=binary_format)

        # è½¬æ¢ä¸º PyTorch æ ¼å¼
        wv_tokens = list(wv_model.index_to_key)  # è·å–å•è¯è¡¨
        wv_arr = torch.tensor(wv_model.vectors, dtype=torch.float32)  # è¯å‘é‡çŸ©é˜µ
        wv_size = wv_model.vector_size  # è¯å‘é‡ç»´åº¦
        print(f"ğŸ”¹ åŠ è½½çš„è¯å‘é‡ç»´åº¦: {wv_size}")  # æ‰“å°è¯å‘é‡ç»´åº¦
        wv_dict = {word: i for i, word in enumerate(wv_tokens)}  # å•è¯ â†’ ç´¢å¼•æ˜ å°„

        # ä¿å­˜ä¸º .pt ä»¥åŠ å¿«ä¸‹æ¬¡åŠ è½½
        torch.save((wv_dict, wv_arr, wv_size), fname_pt)
        print(f"è¯å‘é‡è½¬æ¢å®Œæˆï¼Œå¹¶ä¿å­˜ä¸º .pt: {fname_pt}")

        return wv_dict, wv_arr, wv_size
    except Exception as e:
        raise RuntimeError(f" è§£æ .bin æˆ– .txt å¤±è´¥: {e}")


def obj_edge_vectors(names, wv_dir, wv_type='GoogleNews-vectors-negative300', wv_dim=300):
    """
    ä¸ºç»™å®šçš„ç‰©ä½“ç±»åˆ«åç§°åŠ è½½ Word2Vec è¯å‘é‡ã€‚
    :param names: ç‰©ä½“ç±»åˆ«åç§°åˆ—è¡¨ã€‚
    :param wv_dir: è¯å‘é‡æ–‡ä»¶çš„å­˜å‚¨ç›®å½•ã€‚
    :param wv_type: è¯å‘é‡ç±»å‹ï¼ˆæ–‡ä»¶åï¼Œä¸å«æ‰©å±•åï¼‰ã€‚
    :param wv_dim: è¯å‘é‡çš„ç»´åº¦ã€‚
    :return: è¯å‘é‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º [len(names), wv_dim]ã€‚
    """
    # åŠ è½½è¯å‘é‡
    wv_dict, wv_arr, wv_size = load_word_vectors(wv_dir, wv_type, wv_dim)
    print(f"ğŸ”¹ åŠ è½½çš„è¯å‘é‡ç»´åº¦: {wv_size}")  # æ‰“å°è¯å‘é‡ç»´åº¦

    # åˆå§‹åŒ–è¯å‘é‡å¼ é‡
    vectors = torch.randn(len(names), wv_dim, dtype=torch.float32)  # ç›´æ¥åˆå§‹åŒ–

    # ä¸ºæ¯ä¸ªåç§°åŠ è½½è¯å‘é‡
    for i, token in enumerate(names):
        wv_index = wv_dict.get(token, None)
        if wv_index is not None:
            vectors[i] = wv_arr[wv_index]
        else:
            # å°è¯•ç”¨åç§°ä¸­æœ€é•¿çš„å•è¯åŒ¹é…
            lw_token = max(token.split(' '), key=len)  # ç›´æ¥å–æœ€é•¿å•è¯
            print(f"ğŸ”¹ '{token}' -> '{lw_token}' (å°è¯•åŒ¹é…)")

            wv_index = wv_dict.get(lw_token, None)
            if wv_index is not None:
                vectors[i] = wv_arr[wv_index]
            else:
                print(f" æ— æ³•æ‰¾åˆ° '{token}' çš„è¯å‘é‡")

    return vectors

# obj_classes=["cat","dog"]
# word2vec_dir="/home/p_zhuzy/p_zhu/PySGG-main/datasets/vg/stanford_spilt/word2vec"

# if __name__== "__main__":
#     obj_embed_vecs = obj_edge_vectors(
#             obj_classes, wv_dir=word2vec_dir, wv_dim=300
#         )