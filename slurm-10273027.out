æ¨¡å‹åŠ è½½æˆåŠŸï¼
è¯æ±‡è¡¨å¤§å°: 3000000
[03/13 13:03:30 pysgg]: Using 1 GPUs
[03/13 13:03:30 pysgg]: Namespace(config_file='', local_rank=0, num_workers=4, output_dir='checkpoints/sgdet-BGNNPredictor/2025-03-13_13', dataset_name='VG_stanford', dataset_name_test='VG_stanford_test', dataset_name_val='VG_stanford_val', img_dir='datasets/vg/stanford_spilt/VG_100k_images', glove_dir='datasets/vg/stanford_spilt/glove', word2vec_dir='datasets/vg/stanford_spilt/word2vec', roidb_file='datasets/vg/VG-SGG-with-attri.h5', dict_file='datasets/vg/VG-SGG-dicts-with-attri.json', image_file='datasets/vg/image_data.json', debug=True, train_pre_batch=3, test_pre_batch=1, max_iter=1000, size_divisbility=32, base_lr=0.008, weight_decay=1e-05, bias_factor=1.0, weight_decay_bias=0.0, momentum=0.9, pretrained_detection_ckpt='checkpoints/detection/pretrained_faster_rcnn/vg_faster_det.pth', checkpoint_period=500, skip_test=False, opts=[], res_in_channels=64, res_out_channels=256, backbone_out_channels=256, box_head_num_class=151, device='cuda', distributed=False)
[03/13 13:03:30 pysgg]: #################### Start initializing dataset & dataloader ####################
[03/13 13:03:48 dataset]: using resampling method:bilvl
[03/13 13:03:48 dataset]: generate the repeat dict according to hyper_param on the fly
[03/13 13:03:48 dataset]: global repeat factor: 0.012;  
[03/13 13:03:48 dataset]: drop rate: 0.4;
[03/13 13:03:51 dataset]: using resampling method:bilvl
[03/13 13:03:51 dataset]: generate the repeat dict according to hyper_param on the fly
[03/13 13:03:51 dataset]: global repeat factor: 0.012;  
[03/13 13:03:51 dataset]: drop rate: 0.4;
[03/13 13:03:54 utils.miscellaneous]: Saving labels mapping into checkpoints/sgdet-BGNNPredictor/2025-03-13_13/labels.json
/home/p_zhuzy/miniconda3/envs/pysgg/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[03/13 13:03:55 pysgg]: the nums of train loader is 1000
[03/13 13:03:55 pysgg]: the nums of val loader is 600
[03/13 13:03:55 pysgg]: #################### end dataloader ####################
[03/13 13:03:55 pysgg]: #################### prepare training ####################
ğŸ”¹ ç›´æ¥åŠ è½½ PyTorch è¯å‘é‡: datasets/vg/stanford_spilt/word2vec/GoogleNews-vectors-negative300.300d.pt
ğŸ”¹ åŠ è½½çš„è¯å‘é‡ç»´åº¦: 300
ğŸ”¹ '__background__' -> '__background__' (å°è¯•åŒ¹é…)
 æ— æ³•æ‰¾åˆ° '__background__' çš„è¯å‘é‡
ğŸ”¹ ç›´æ¥åŠ è½½ PyTorch è¯å‘é‡: datasets/vg/stanford_spilt/word2vec/GoogleNews-vectors-negative300.200d.pt
ğŸ”¹ åŠ è½½çš„è¯å‘é‡ç»´åº¦: 300
ğŸ”¹ '__background__' -> '__background__' (å°è¯•åŒ¹é…)
 æ— æ³•æ‰¾åˆ° '__background__' çš„è¯å‘é‡
Traceback (most recent call last):
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 561, in <module>
    main()
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 553, in main
    model = train(args,args.local_rank, args.distributed,logger)
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 139, in train
    model = build_detection_model(args)
  File "/project/p_zhu/PySGG-main/util/model.py", line 63, in build_detection_model
    return GeneralizedRCNN(args)
  File "/project/p_zhu/PySGG-main/util/model.py", line 25, in __init__
    self.roi_heads = build_roi_heads(args, self.backbone.out_channels)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2329, in build_roi_heads
    roi_heads.append(("relation", build_roi_relation_head(args, in_channels)))
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2306, in build_roi_relation_head
    return ROIRelationHead(args,in_channels)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2107, in __init__
    self.predictor = BGNNPredictor(args, feat_dim)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 1774, in __init__
    self.context_layer = BGNNContext(
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 1002, in __init__
    RelAwareRelFeature(args,input_dim)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 816, in __init__
    obj_embed_vecs = obj_edge_vectors(
  File "/project/p_zhu/PySGG-main/util/load_word2vec.py", line 107, in obj_edge_vectors
    vectors[i] = wv_arr[wv_index]
RuntimeError: The expanded size of the tensor (200) must match the existing size (300) at non-singleton dimension 0.  Target sizes: [200].  Tensor sizes: [300]
