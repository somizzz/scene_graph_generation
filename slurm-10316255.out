[03/16 15:43:36 pysgg]: Using 1 GPUs
[03/16 15:43:36 pysgg]: Namespace(config_file='', local_rank=0, num_workers=4, output_dir='checkpoints/sgdet-BGNNPredictor/2025-03-16_15', dataset_name='VG_stanford', dataset_name_test='VG_stanford_test', dataset_name_val='VG_stanford_val', img_dir='datasets/vg/stanford_spilt/VG_100k_images', bert_dir='datasets/vg/stanford_spilt/bert', roidb_file='datasets/vg/VG-SGG-with-attri.h5', dict_file='datasets/vg/VG-SGG-dicts-with-attri.json', image_file='datasets/vg/image_data.json', debug=False, train_pre_batch=4, test_pre_batch=1, max_iter=1000, size_divisbility=32, base_lr=0.008, weight_decay=1e-05, bias_factor=1.0, weight_decay_bias=0.0, momentum=0.9, pretrained_detection_ckpt='checkpoints/detection/pretrained_faster_rcnn/vg_faster_det.pth', checkpoint_period=500, skip_test=False, opts=[], res_in_channels=64, res_out_channels=256, backbone_out_channels=256, box_head_num_class=151, device='cuda', distributed=False)
[03/16 15:43:36 pysgg]: #################### Start initializing dataset & dataloader ####################
[03/16 15:44:14 dataset]: using resampling method:bilvl
[03/16 15:44:14 dataset]: load repeat_dict from checkpoints/sgdet-BGNNPredictor/2025-03-16_15/repeat_dict.pkl
[03/16 15:44:14 dataset]: using resampling method:bilvl
[03/16 15:44:14 dataset]: load repeat_dict from checkpoints/sgdet-BGNNPredictor/2025-03-16_15/repeat_dict.pkl
[03/16 15:44:14 utils.miscellaneous]: Saving labels mapping into checkpoints/sgdet-BGNNPredictor/2025-03-16_15/labels.json
/home/p_zhuzy/miniconda3/envs/pysgg/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[03/16 15:44:16 pysgg]: the nums of train loader is 1000
[03/16 15:44:16 pysgg]: the nums of val loader is 3881
[03/16 15:44:16 pysgg]: #################### end dataloader ####################
[03/16 15:44:16 pysgg]: #################### prepare training ####################
Some weights of BertModel were not initialized from the model checkpoint at datasets/vg/stanford_spilt/bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 476, in <module>
    main()
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 468, in main
    model = train(args,args.local_rank, args.distributed,logger)
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 132, in train
    model = build_detection_model(args)
  File "/project/p_zhu/PySGG-main/util/model.py", line 63, in build_detection_model
    return GeneralizedRCNN(args)
  File "/project/p_zhu/PySGG-main/util/model.py", line 25, in __init__
    self.roi_heads = build_roi_heads(args, self.backbone.out_channels)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2408, in build_roi_heads
    roi_heads.append(("relation", build_roi_relation_head(args, in_channels)))
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2341, in build_roi_relation_head
    return ROIRelationHead(args,in_channels)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2137, in __init__
    self.predictor = BGNNPredictor(args, feat_dim)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 1793, in __init__
    self.context_layer = BGNNContext(
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 1002, in __init__
    self.pairwise_feature_extractor = PairwiseFeatureExtractor(args, in_channels)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 659, in __init__
    obj_embed_vecs = obj_edge_vectors_bert(self.obj_classes, wv_dir=args.bert_dir, wv_dim=self.embed_dim)
  File "/project/p_zhu/PySGG-main/util/load_bert.py", line 48, in obj_edge_vectors_bert
    vectors[i] = cls_embedding.squeeze(0)
RuntimeError: The expanded size of the tensor (300) must match the existing size (768) at non-singleton dimension 0.  Target sizes: [300].  Tensor sizes: [768]
