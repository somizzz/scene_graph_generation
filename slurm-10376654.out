[03/22 00:53:48 pysgg]: Using 1 GPUs
[03/22 00:53:48 pysgg]: Namespace(config_file='', local_rank=0, num_workers=4, output_dir='checkpoints/sgdet-BGNNPredictor/2025-03-22_00', dataset_name='VG_stanford', dataset_name_test='VG_stanford_test', dataset_name_val='VG_stanford_val', img_dir='datasets/vg/stanford_spilt/VG_100k_images', bert_dir='datasets/vg/stanford_spilt/bert', roidb_file='datasets/vg/VG-SGG-with-attri.h5', dict_file='datasets/vg/VG-SGG-dicts-with-attri.json', image_file='datasets/vg/image_data.json', debug=True, train_pre_batch=4, test_pre_batch=1, max_iter=1000, size_divisbility=32, base_lr=0.008, weight_decay=1e-05, bias_factor=1.0, weight_decay_bias=0.0, momentum=0.9, pretrained_detection_ckpt='checkpoints/detection/pretrained_faster_rcnn/vg_faster_det.pth', checkpoint_period=500, skip_test=False, opts=[], res_in_channels=64, res_out_channels=256, backbone_out_channels=256, box_head_num_class=151, device='cuda', distributed=False)
[03/22 00:53:48 pysgg]: #################### Start initializing dataset & dataloader ####################
[03/22 00:54:09 dataset]: using resampling method:bilvl
[03/22 00:54:09 dataset]: generate the repeat dict according to hyper_param on the fly
[03/22 00:54:09 dataset]: global repeat factor: 0.1;  
[03/22 00:54:09 dataset]: drop rate: 0.9;
[03/22 00:54:11 dataset]: using resampling method:bilvl
[03/22 00:54:11 dataset]: load repeat_dict from checkpoints/sgdet-BGNNPredictor/2025-03-22_00/repeat_dict.pkl
[03/22 00:54:11 utils.miscellaneous]: Saving labels mapping into checkpoints/sgdet-BGNNPredictor/2025-03-22_00/labels.json
/home/p_zhuzy/miniconda3/envs/pysgg/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[03/22 00:54:12 pysgg]: the nums of train loader is 1000
[03/22 00:54:12 pysgg]: the nums of val loader is 474
[03/22 00:54:12 pysgg]: #################### end dataloader ####################
[03/22 00:54:12 pysgg]: #################### prepare training ####################
[03/22 00:54:18 dataset]: using resampling method:bilvl
[03/22 00:54:18 dataset]: load repeat_dict from checkpoints/sgdet-BGNNPredictor/2025-03-22_00/repeat_dict.pkl
[03/22 00:54:18 dataset]: using resampling method:bilvl
[03/22 00:54:18 dataset]: load repeat_dict from checkpoints/sgdet-BGNNPredictor/2025-03-22_00/repeat_dict.pkl
Get relation class frequency distribution on dataset.
  0%|          | 0/8915 [00:00<?, ?it/s]  5%|▍         | 421/8915 [00:00<00:02, 4202.94it/s] 10%|▉         | 879/8915 [00:00<00:01, 4419.08it/s] 15%|█▍        | 1332/8915 [00:00<00:01, 4468.59it/s] 20%|█▉        | 1779/8915 [00:00<00:01, 4459.60it/s] 25%|██▌       | 2243/8915 [00:00<00:01, 4523.60it/s] 30%|███       | 2717/8915 [00:00<00:01, 4595.00it/s] 36%|███▌      | 3177/8915 [00:00<00:01, 4465.25it/s] 41%|████      | 3625/8915 [00:00<00:01, 4433.17it/s] 46%|████▌     | 4069/8915 [00:00<00:01, 4392.06it/s] 51%|█████     | 4523/8915 [00:01<00:00, 4433.43it/s] 56%|█████▌    | 4997/8915 [00:01<00:00, 4521.89it/s] 61%|██████    | 5457/8915 [00:01<00:00, 4542.93it/s] 66%|██████▋   | 5912/8915 [00:01<00:00, 4442.07it/s] 71%|███████▏  | 6357/8915 [00:01<00:00, 4440.42it/s] 76%|███████▋  | 6802/8915 [00:01<00:00, 4133.55it/s] 81%|████████  | 7220/8915 [00:01<00:00, 4057.38it/s] 86%|████████▌ | 7629/8915 [00:01<00:00, 3938.40it/s] 90%|█████████ | 8026/8915 [00:01<00:00, 3815.36it/s] 94%|█████████▍| 8410/8915 [00:01<00:00, 3819.97it/s] 99%|█████████▊| 8794/8915 [00:02<00:00, 3816.04it/s]100%|██████████| 8915/8915 [00:02<00:00, 4216.43it/s]
  0%|          | 0/4079 [00:00<?, ?it/s] 20%|██        | 827/4079 [00:00<00:00, 8264.02it/s] 41%|████      | 1654/4079 [00:00<00:00, 7500.02it/s] 61%|██████    | 2474/4079 [00:00<00:00, 7799.51it/s] 80%|███████▉  | 3259/4079 [00:00<00:00, 7622.31it/s] 99%|█████████▊| 4024/4079 [00:00<00:00, 7324.84it/s]100%|██████████| 4079/4079 [00:00<00:00, 7502.11it/s]
  0%|          | 0/8915 [00:00<?, ?it/s] 16%|█▋        | 1456/8915 [00:00<00:00, 14558.52it/s] 33%|███▎      | 2912/8915 [00:00<00:00, 14556.19it/s] 49%|████▉     | 4368/8915 [00:00<00:00, 13895.78it/s] 65%|██████▌   | 5797/8915 [00:00<00:00, 14045.64it/s] 81%|████████  | 7205/8915 [00:00<00:00, 13509.13it/s] 96%|█████████▌| 8561/8915 [00:00<00:00, 13148.04it/s]100%|██████████| 8915/8915 [00:00<00:00, 13497.56it/s]
Some weights of BertModel were not initialized from the model checkpoint at datasets/vg/stanford_spilt/bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at datasets/vg/stanford_spilt/bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at datasets/vg/stanford_spilt/bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at datasets/vg/stanford_spilt/bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[03/22 00:54:46 pysgg]: #################### end model construction ####################
[03/22 00:54:46 pysgg]: GeneralizedRCNN(
  (backbone): Sequential(
    (body): ResNet(
      (stem): StemWithFixedBatchNorm(
        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (bn1): FrozenBatchNorm2d()
      )
      (layer1): Sequential(
        (0): BottleneckWithFixedBatchNorm(
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): FrozenBatchNorm2d()
          )
          (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (1): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (2): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
      )
      (layer2): Sequential(
        (0): BottleneckWithFixedBatchNorm(
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d()
          )
          (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (1): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (2): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (3): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
      )
      (layer3): Sequential(
        (0): BottleneckWithFixedBatchNorm(
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d()
          )
          (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (1): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (2): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (3): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (4): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (5): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (6): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (7): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (8): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (9): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (10): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (11): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (12): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (13): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (14): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (15): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (16): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (17): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (18): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (19): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (20): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (21): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (22): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
      )
      (layer4): Sequential(
        (0): BottleneckWithFixedBatchNorm(
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d()
          )
          (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (1): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (2): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
      )
    )
    (fpn): FPN(
      (fpn_inner1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (fpn_layer1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (fpn_inner2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (fpn_layer2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (fpn_inner3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      (fpn_layer3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (fpn_inner4): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (fpn_layer4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (top_blocks): LastLevelMaxPool()
    )
  )
  (rpn): RPNModule(
    (anchor_generator): AnchorGenerator(
      (cell_anchors): BufferList()
    )
    (head): RPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cls_logits): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))
      (bbox_pred): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
    )
    (box_selector_train): RPNPostProcessor()
    (box_selector_test): RPNPostProcessor()
  )
  (roi_heads): CombinedROIHeads(
    (box): ROIBoxHead(
      (feature_extractor): FPN2MLPFeatureExtractor(
        (pooler): Pooler(
          (poolers): ModuleList(
            (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=2)
            (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=2)
            (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=2)
            (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=2)
          )
        )
        (fc6): Linear(in_features=12544, out_features=4096, bias=True)
        (fc7): Linear(in_features=4096, out_features=4096, bias=True)
      )
      (predictor): FPNPredictor(
        (cls_score): Linear(in_features=4096, out_features=151, bias=True)
        (bbox_pred): Linear(in_features=4096, out_features=604, bias=True)
      )
      (post_processor): PostProcessor()
    )
    (relation): ROIRelationHead(
      (union_feature_extractor): RelationFeatureExtractor(
        (feature_extractor): FPN2MLPFeatureExtractor(
          (pooler): Pooler(
            (poolers): ModuleList(
              (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=2)
              (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=2)
              (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=2)
              (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=2)
            )
            (reduce_channel): Sequential(
              (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): ReLU(inplace=True)
            )
          )
          (fc6): Linear(in_features=12544, out_features=4096, bias=True)
          (fc7): Linear(in_features=4096, out_features=4096, bias=True)
        )
        (rect_conv): Sequential(
          (0): Conv2d(2, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
          (1): ReLU(inplace=True)
          (2): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
          (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): ReLU(inplace=True)
          (6): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (box_feature_extractor): FPN2MLPFeatureExtractor(
        (pooler): Pooler(
          (poolers): ModuleList(
            (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=2)
            (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=2)
            (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=2)
            (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=2)
          )
        )
        (fc6): Linear(in_features=12544, out_features=4096, bias=True)
        (fc7): Linear(in_features=4096, out_features=4096, bias=True)
      )
      (predictor): BGNNPredictor(
        (context_layer): BGNNContext(
          (pairwise_feature_extractor): PairwiseFeatureExtractor(
            (obj_embed_on_prob_dist): Embedding(151, 768)
            (obj_embed_on_pred_label): Embedding(151, 768)
            (rel_feature_up_dim): Linear(in_features=4096, out_features=2048, bias=True)
            (pairwise_obj_feat_updim_fc): Linear(in_features=5376, out_features=1024, bias=True)
            (pos_embed): Sequential(
              (0): Linear(in_features=9, out_features=32, bias=True)
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
              (2): Linear(in_features=32, out_features=128, bias=True)
              (3): ReLU(inplace=True)
            )
            (spt_emb): Sequential(
              (0): Linear(in_features=32, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=1024, bias=True)
              (3): ReLU(inplace=True)
            )
            (pairwise_rel_feat_finalize_fc): Sequential(
              (0): Linear(in_features=1024, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
            )
            (obj_hidden_linear): Linear(in_features=4992, out_features=512, bias=True)
            (obj_feat_aug_finalize_fc): Sequential(
              (0): Linear(in_features=5376, out_features=2048, bias=True)
              (1): ReLU(inplace=True)
            )
          )
          (relation_conf_aware_models): ModuleList(
            (0): RelAwareRelFeature(
              (obj_sem_embed): Embedding(151, 768)
              (obj_pos_embed): Sequential(
                (0): Linear(in_features=9, out_features=128, bias=True)
                (1): ReLU()
                (2): Linear(in_features=128, out_features=128, bias=True)
              )
              (proposal_box_feat_extract): Sequential(
                (0): ReLU()
                (1): Linear(in_features=1792, out_features=512, bias=True)
              )
              (vis_embed): Sequential(
                (0): ReLU()
                (1): Linear(in_features=2048, out_features=512, bias=True)
              )
              (proposal_feat_fusion): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=512, bias=True)
              )
              (proposal_relness_cls_fc): Sequential(
                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=50, bias=True)
              )
              (fusion_layer): Linear(in_features=50, out_features=1, bias=True)
            )
            (1): RelAwareRelFeature(
              (obj_sem_embed): Embedding(151, 768)
              (obj_pos_embed): Sequential(
                (0): Linear(in_features=9, out_features=128, bias=True)
                (1): ReLU()
                (2): Linear(in_features=128, out_features=128, bias=True)
              )
              (proposal_box_feat_extract): Sequential(
                (0): ReLU()
                (1): Linear(in_features=1792, out_features=512, bias=True)
              )
              (vis_embed): Sequential(
                (0): ReLU()
                (1): Linear(in_features=512, out_features=512, bias=True)
              )
              (proposal_feat_fusion): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=512, bias=True)
              )
              (proposal_relness_cls_fc): Sequential(
                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=50, bias=True)
              )
              (fusion_layer): Linear(in_features=50, out_features=1, bias=True)
            )
            (2): RelAwareRelFeature(
              (obj_sem_embed): Embedding(151, 768)
              (obj_pos_embed): Sequential(
                (0): Linear(in_features=9, out_features=128, bias=True)
                (1): ReLU()
                (2): Linear(in_features=128, out_features=128, bias=True)
              )
              (proposal_box_feat_extract): Sequential(
                (0): ReLU()
                (1): Linear(in_features=1792, out_features=512, bias=True)
              )
              (vis_embed): Sequential(
                (0): ReLU()
                (1): Linear(in_features=512, out_features=512, bias=True)
              )
              (proposal_feat_fusion): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=512, bias=True)
              )
              (proposal_relness_cls_fc): Sequential(
                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=50, bias=True)
              )
              (fusion_layer): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (learnable_relness_score_gating_recalibration): LearnableRelatednessGating()
          (obj_downdim_fc): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
            (1): ReLU(inplace=True)
          )
          (rel_downdim_fc): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
            (1): ReLU(inplace=True)
          )
          (obj_pair2rel_fuse): Sequential(
            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): ReLU()
          )
          (gate_sub2pred): Sequential(
            (0): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
            (1): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
            (2): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
          )
          (gate_obj2pred): Sequential(
            (0): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
            (1): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
            (2): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
          )
          (gate_pred2sub): Sequential(
            (0): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
            (1): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
            (2): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
          )
          (gate_pred2obj): Sequential(
            (0): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
            (1): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
            (2): MessagePassingUnit_v1(
              (w): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): ReLU()
                (2): Linear(in_features=1024, out_features=128, bias=True)
              )
            )
          )
          (object_msg_fusion): Sequential(
            (0): MessageFusion(
              (wih): Linear(in_features=512, out_features=512, bias=True)
              (whh): Linear(in_features=512, out_features=512, bias=True)
            )
            (1): MessageFusion(
              (wih): Linear(in_features=512, out_features=512, bias=True)
              (whh): Linear(in_features=512, out_features=512, bias=True)
            )
            (2): MessageFusion(
              (wih): Linear(in_features=512, out_features=512, bias=True)
              (whh): Linear(in_features=512, out_features=512, bias=True)
            )
          )
          (pred_msg_fusion): Sequential(
            (0): MessageFusion(
              (wih): Linear(in_features=512, out_features=512, bias=True)
              (whh): Linear(in_features=512, out_features=512, bias=True)
            )
            (1): MessageFusion(
              (wih): Linear(in_features=512, out_features=512, bias=True)
              (whh): Linear(in_features=512, out_features=512, bias=True)
            )
            (2): MessageFusion(
              (wih): Linear(in_features=512, out_features=512, bias=True)
              (whh): Linear(in_features=512, out_features=512, bias=True)
            )
          )
        )
        (rel_classifier): DotProductClassifier()
        (obj_classifier): DotProductClassifier()
        (rel_aware_loss_eval): RelAwareLoss()
        (freq_bias): FrequencyBias(
          (obj_baseline): Embedding(22801, 51)
        )
      )
      (post_processor): PostProcessor_Relation()
    )
  )
)
[03/22 00:54:46 pysgg]: trainable models:
[03/22 00:54:46 pysgg]: 
backbone.body.stem.conv1.weight                                                 : [64,3,7,7]      (    9408) (    )
backbone.body.layer1.0.downsample.0.weight                                      : [256,64,1,1]    (   16384) (    )
backbone.body.layer1.0.conv1.weight                                             : [256,64,1,1]    (   16384) (    )
backbone.body.layer1.0.conv2.weight                                             : [256,8,3,3]     (   18432) (    )
backbone.body.layer1.0.conv3.weight                                             : [256,256,1,1]   (   65536) (    )
backbone.body.layer1.1.conv1.weight                                             : [256,256,1,1]   (   65536) (    )
backbone.body.layer1.1.conv2.weight                                             : [256,8,3,3]     (   18432) (    )
backbone.body.layer1.1.conv3.weight                                             : [256,256,1,1]   (   65536) (    )
backbone.body.layer1.2.conv1.weight                                             : [256,256,1,1]   (   65536) (    )
backbone.body.layer1.2.conv2.weight                                             : [256,8,3,3]     (   18432) (    )
backbone.body.layer1.2.conv3.weight                                             : [256,256,1,1]   (   65536) (    )
backbone.body.layer2.0.downsample.0.weight                                      : [512,256,1,1]   (  131072) (    )
backbone.body.layer2.0.conv1.weight                                             : [512,256,1,1]   (  131072) (    )
backbone.body.layer2.0.conv2.weight                                             : [512,16,3,3]    (   73728) (    )
backbone.body.layer2.0.conv3.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.1.conv1.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.1.conv2.weight                                             : [512,16,3,3]    (   73728) (    )
backbone.body.layer2.1.conv3.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.2.conv1.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.2.conv2.weight                                             : [512,16,3,3]    (   73728) (    )
backbone.body.layer2.2.conv3.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.3.conv1.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer2.3.conv2.weight                                             : [512,16,3,3]    (   73728) (    )
backbone.body.layer2.3.conv3.weight                                             : [512,512,1,1]   (  262144) (    )
backbone.body.layer3.0.downsample.0.weight                                      : [1024,512,1,1]  (  524288) (    )
backbone.body.layer3.0.conv1.weight                                             : [1024,512,1,1]  (  524288) (    )
backbone.body.layer3.0.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.0.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.1.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.1.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.1.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.2.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.2.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.2.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.3.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.3.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.3.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.4.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.4.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.4.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.5.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.5.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.5.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.6.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.6.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.6.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.7.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.7.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.7.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.8.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.8.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.8.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.9.conv1.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.9.conv2.weight                                             : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.9.conv3.weight                                             : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.10.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.10.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.10.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.11.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.11.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.11.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.12.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.12.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.12.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.13.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.13.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.13.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.14.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.14.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.14.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.15.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.15.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.15.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.16.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.16.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.16.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.17.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.17.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.17.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.18.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.18.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.18.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.19.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.19.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.19.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.20.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.20.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.20.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.21.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.21.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.21.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.22.conv1.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer3.22.conv2.weight                                            : [1024,32,3,3]   (  294912) (    )
backbone.body.layer3.22.conv3.weight                                            : [1024,1024,1,1] ( 1048576) (    )
backbone.body.layer4.0.downsample.0.weight                                      : [2048,1024,1,1] ( 2097152) (    )
backbone.body.layer4.0.conv1.weight                                             : [2048,1024,1,1] ( 2097152) (    )
backbone.body.layer4.0.conv2.weight                                             : [2048,64,3,3]   ( 1179648) (    )
backbone.body.layer4.0.conv3.weight                                             : [2048,2048,1,1] ( 4194304) (    )
backbone.body.layer4.1.conv1.weight                                             : [2048,2048,1,1] ( 4194304) (    )
backbone.body.layer4.1.conv2.weight                                             : [2048,64,3,3]   ( 1179648) (    )
backbone.body.layer4.1.conv3.weight                                             : [2048,2048,1,1] ( 4194304) (    )
backbone.body.layer4.2.conv1.weight                                             : [2048,2048,1,1] ( 4194304) (    )
backbone.body.layer4.2.conv2.weight                                             : [2048,64,3,3]   ( 1179648) (    )
backbone.body.layer4.2.conv3.weight                                             : [2048,2048,1,1] ( 4194304) (    )
backbone.fpn.fpn_inner1.weight                                                  : [256,256,1,1]   (   65536) (    )
backbone.fpn.fpn_layer1.weight                                                  : [256,256,3,3]   (  589824) (    )
backbone.fpn.fpn_inner2.weight                                                  : [256,512,1,1]   (  131072) (    )
backbone.fpn.fpn_layer2.weight                                                  : [256,256,3,3]   (  589824) (    )
backbone.fpn.fpn_inner3.weight                                                  : [256,1024,1,1]  (  262144) (    )
backbone.fpn.fpn_layer3.weight                                                  : [256,256,3,3]   (  589824) (    )
backbone.fpn.fpn_inner4.weight                                                  : [256,2048,1,1]  (  524288) (    )
backbone.fpn.fpn_layer4.weight                                                  : [256,256,3,3]   (  589824) (    )
rpn.head.conv.weight                                                            : [256,256,3,3]   (  589824) (    )
rpn.head.cls_logits.weight                                                      : [4,256,1,1]     (    1024) (    )
rpn.head.bbox_pred.weight                                                       : [16,256,1,1]    (    4096) (    )
roi_heads.box.feature_extractor.fc6.weight                                      : [4096,12544]    (51380224) (    )
roi_heads.box.feature_extractor.fc7.weight                                      : [4096,4096]     (16777216) (    )
roi_heads.box.predictor.cls_score.weight                                        : [151,4096]      (  618496) (    )
roi_heads.box.predictor.bbox_pred.weight                                        : [604,4096]      ( 2473984) (    )
roi_heads.relation.rel_pn_thres                                                 : [1]             (       1) (    )
roi_heads.relation.rel_pn_thres_for_test                                        : [1]             (       1) (    )
roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: [256,1024,3,3]  ( 2359296) (grad)
roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight         : [4096,12544]    (51380224) (grad)
roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight         : [4096,4096]     (16777216) (grad)
roi_heads.relation.union_feature_extractor.rect_conv.0.weight                   : [128,2,7,7]     (   12544) (grad)
roi_heads.relation.union_feature_extractor.rect_conv.2.weight                   : [128]           (     128) (grad)
roi_heads.relation.union_feature_extractor.rect_conv.4.weight                   : [256,128,3,3]   (  294912) (grad)
roi_heads.relation.union_feature_extractor.rect_conv.6.weight                   : [256]           (     256) (grad)
roi_heads.relation.box_feature_extractor.fc6.weight                             : [4096,12544]    (51380224) (grad)
roi_heads.relation.box_feature_extractor.fc7.weight                             : [4096,4096]     (16777216) (grad)
roi_heads.relation.predictor.freq_lambda                                        : [1]             (       1) (    )
roi_heads.relation.predictor.context_layer.padding_feature                      : [512]           (     512) (    )
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_embed_on_prob_dist.weight: [151,768]       (  115968) (grad)
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_embed_on_pred_label.weight: [151,768]       (  115968) (grad)
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.rel_feature_up_dim.weight: [2048,4096]     ( 8388608) (grad)
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pairwise_obj_feat_updim_fc.weight: [1024,5376]     ( 5505024) (grad)
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.0.weight: [32,9]          (     288) (grad)
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.1.weight: [32]            (      32) (grad)
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.2.weight: [128,32]        (    4096) (grad)
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.spt_emb.0.weight: [512,32]        (   16384) (grad)
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.spt_emb.2.weight: [1024,512]      (  524288) (grad)
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pairwise_rel_feat_finalize_fc.0.weight: [2048,1024]     ( 2097152) (grad)
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_hidden_linear.weight: [512,4992]      ( 2555904) (grad)
roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_feat_aug_finalize_fc.0.weight: [2048,5376]     (11010048) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_sem_embed.weight: [151,768]       (  115968) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_pos_embed.0.weight: [128,9]         (    1152) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_pos_embed.2.weight: [128,128]       (   16384) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_box_feat_extract.1.weight: [512,1792]      (  917504) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.vis_embed.1.weight: [512,2048]      ( 1048576) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_feat_fusion.0.weight: [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_feat_fusion.2.weight: [512,1024]      (  524288) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_relness_cls_fc.0.weight: [512]           (     512) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_relness_cls_fc.2.weight: [50,512]        (   25600) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.fusion_layer.weight: [1,50]          (      50) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_sem_embed.weight: [151,768]       (  115968) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_pos_embed.0.weight: [128,9]         (    1152) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_pos_embed.2.weight: [128,128]       (   16384) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_box_feat_extract.1.weight: [512,1792]      (  917504) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.vis_embed.1.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_feat_fusion.0.weight: [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_feat_fusion.2.weight: [512,1024]      (  524288) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_relness_cls_fc.0.weight: [512]           (     512) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_relness_cls_fc.2.weight: [50,512]        (   25600) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.fusion_layer.weight: [1,50]          (      50) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_sem_embed.weight: [151,768]       (  115968) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_pos_embed.0.weight: [128,9]         (    1152) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_pos_embed.2.weight: [128,128]       (   16384) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_box_feat_extract.1.weight: [512,1792]      (  917504) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.vis_embed.1.weight: [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_feat_fusion.0.weight: [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_feat_fusion.2.weight: [512,1024]      (  524288) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_relness_cls_fc.0.weight: [512]           (     512) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_relness_cls_fc.2.weight: [50,512]        (   25600) (grad)
roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.fusion_layer.weight: [1,50]          (      50) (grad)
roi_heads.relation.predictor.context_layer.learnable_relness_score_gating_recalibration.alpha: [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.learnable_relness_score_gating_recalibration.beta: [1]             (       1) (    )
roi_heads.relation.predictor.context_layer.obj_downdim_fc.0.weight              : [512,2048]      ( 1048576) (grad)
roi_heads.relation.predictor.context_layer.rel_downdim_fc.0.weight              : [512,2048]      ( 1048576) (grad)
roi_heads.relation.predictor.context_layer.obj_pair2rel_fuse.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.obj_pair2rel_fuse.1.weight           : [512,1024]      (  524288) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.0.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.0.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.0.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.0.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.1.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.1.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.1.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.1.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.2.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.2.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.2.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_sub2pred.2.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.0.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.0.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.0.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.0.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.1.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.1.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.1.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.1.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.2.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.2.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.2.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_obj2pred.2.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.0.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.0.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.0.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.0.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.1.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.1.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.1.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.1.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.2.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.2.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.2.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2sub.2.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.0.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.0.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.0.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.0.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.1.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.1.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.1.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.1.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.2.gate_weight          : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.2.aux_gate_weight      : [1]             (       1) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.2.w.0.weight           : [1024]          (    1024) (grad)
roi_heads.relation.predictor.context_layer.gate_pred2obj.2.w.2.weight           : [128,1024]      (  131072) (grad)
roi_heads.relation.predictor.context_layer.object_msg_fusion.0.wih.weight       : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.object_msg_fusion.0.whh.weight       : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.object_msg_fusion.1.wih.weight       : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.object_msg_fusion.1.whh.weight       : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.object_msg_fusion.2.wih.weight       : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.object_msg_fusion.2.whh.weight       : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.pred_msg_fusion.0.wih.weight         : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.pred_msg_fusion.0.whh.weight         : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.pred_msg_fusion.1.wih.weight         : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.pred_msg_fusion.1.whh.weight         : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.pred_msg_fusion.2.wih.weight         : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.context_layer.pred_msg_fusion.2.whh.weight         : [512,512]       (  262144) (grad)
roi_heads.relation.predictor.rel_classifier.weight                              : [51,512]        (   26112) (grad)
roi_heads.relation.predictor.obj_classifier.weight                              : [151,512]       (   77312) (grad)
roi_heads.relation.predictor.freq_bias.obj_baseline.weight                      : [22801,51]      ( 1162851) (grad)
 ----- 
 
      trainable parameters:  184.375/346.114 M 
 
load model to GPU
[03/22 00:55:10 pysgg]: #################### end optimizer and shcedule ####################
[03/22 00:55:10 utils.checkpoint]: Loading parameters from checkpoints/detection/pretrained_faster_rcnn/vg_faster_det.pth
[03/22 00:55:11 utils.model_serialization]: MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
[03/22 00:55:11 utils.model_serialization]: MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
[03/22 00:55:11 utils.model_serialization]: MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
[03/22 00:55:11 utils.model_serialization]: MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
[03/22 00:55:11 utils.model_serialization]: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
[03/22 00:55:11 utils.model_serialization]: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
[03/22 00:55:11 utils.model_serialization]: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
[03/22 00:55:11 utils.model_serialization]: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
[03/22 00:55:11 utils.model_serialization]: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
[03/22 00:55:11 utils.model_serialization]: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
[03/22 00:55:11 utils.model_serialization]: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                            loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
[03/22 00:55:11 utils.model_serialization]: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                          loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
[03/22 00:55:11 utils.model_serialization]: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                            loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
[03/22 00:55:11 utils.model_serialization]: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                          loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.0.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.0.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.0.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.0.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.0.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.0.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.1.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.1.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.1.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.1.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.1.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.1.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.2.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.2.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.2.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.2.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.2.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_obj2pred.2.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.0.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.0.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.0.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.0.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.0.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.0.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.1.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.1.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.1.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.1.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.1.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.1.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.2.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.2.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.2.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.2.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.2.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2obj.2.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.0.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.0.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.0.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.0.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.0.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.0.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.1.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.1.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.1.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.1.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.1.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.1.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.2.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.2.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.2.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.2.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.2.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_pred2sub.2.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.0.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.0.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.0.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.0.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.0.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.0.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.1.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.1.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.1.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.1.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.1.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.1.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.2.aux_gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.2.gate_weight of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.2.w.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.2.w.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.2.w.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.gate_sub2pred.2.w.2.weight of shape (128, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.learnable_relness_score_gating_recalibration.alpha of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.learnable_relness_score_gating_recalibration.beta of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_downdim_fc.0.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_downdim_fc.0.weight of shape (512, 2048)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_pair2rel_fuse.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_pair2rel_fuse.0.num_batches_tracked of shape ()
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_pair2rel_fuse.0.running_mean of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_pair2rel_fuse.0.running_var of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_pair2rel_fuse.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_pair2rel_fuse.1.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_pair2rel_fuse.1.weight of shape (512, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.0.whh.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.0.whh.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.0.wih.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.0.wih.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.1.whh.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.1.whh.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.1.wih.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.1.wih.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.2.whh.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.2.whh.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.2.wih.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.object_msg_fusion.2.wih.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.padding_feature of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_embed_on_pred_label.weight of shape (151, 768)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_embed_on_prob_dist.weight of shape (151, 768)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_feat_aug_finalize_fc.0.bias of shape (2048,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_feat_aug_finalize_fc.0.weight of shape (2048, 5376)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_hidden_linear.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_hidden_linear.weight of shape (512, 4992)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pairwise_obj_feat_updim_fc.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pairwise_obj_feat_updim_fc.weight of shape (1024, 5376)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pairwise_rel_feat_finalize_fc.0.bias of shape (2048,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pairwise_rel_feat_finalize_fc.0.weight of shape (2048, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.0.bias of shape (32,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.0.weight of shape (32, 9)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.1.bias of shape (32,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.1.num_batches_tracked of shape ()
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.1.running_mean of shape (32,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.1.running_var of shape (32,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.1.weight of shape (32,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.2.weight of shape (128, 32)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.rel_feature_up_dim.bias of shape (2048,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.rel_feature_up_dim.weight of shape (2048, 4096)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.spt_emb.0.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.spt_emb.0.weight of shape (512, 32)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.spt_emb.2.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.spt_emb.2.weight of shape (1024, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.0.whh.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.0.whh.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.0.wih.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.0.wih.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.1.whh.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.1.whh.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.1.wih.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.1.wih.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.2.whh.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.2.whh.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.2.wih.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pred_msg_fusion.2.wih.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.rel_downdim_fc.0.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.rel_downdim_fc.0.weight of shape (512, 2048)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.fusion_layer.bias of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.fusion_layer.weight of shape (1, 50)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_pos_embed.0.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_pos_embed.0.weight of shape (128, 9)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_pos_embed.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_pos_embed.2.weight of shape (128, 128)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_sem_embed.weight of shape (151, 768)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_box_feat_extract.1.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_box_feat_extract.1.weight of shape (512, 1792)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_feat_fusion.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_feat_fusion.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_feat_fusion.2.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_feat_fusion.2.weight of shape (512, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_relness_cls_fc.0.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_relness_cls_fc.0.weight of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_relness_cls_fc.2.bias of shape (50,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_relness_cls_fc.2.weight of shape (50, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.vis_embed.1.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.vis_embed.1.weight of shape (512, 2048)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.fusion_layer.bias of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.fusion_layer.weight of shape (1, 50)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_pos_embed.0.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_pos_embed.0.weight of shape (128, 9)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_pos_embed.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_pos_embed.2.weight of shape (128, 128)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_sem_embed.weight of shape (151, 768)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_box_feat_extract.1.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_box_feat_extract.1.weight of shape (512, 1792)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_feat_fusion.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_feat_fusion.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_feat_fusion.2.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_feat_fusion.2.weight of shape (512, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_relness_cls_fc.0.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_relness_cls_fc.0.weight of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_relness_cls_fc.2.bias of shape (50,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_relness_cls_fc.2.weight of shape (50, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.vis_embed.1.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.vis_embed.1.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.fusion_layer.bias of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.fusion_layer.weight of shape (1, 50)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_pos_embed.0.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_pos_embed.0.weight of shape (128, 9)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_pos_embed.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_pos_embed.2.weight of shape (128, 128)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_sem_embed.weight of shape (151, 768)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_box_feat_extract.1.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_box_feat_extract.1.weight of shape (512, 1792)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_feat_fusion.0.bias of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_feat_fusion.0.weight of shape (1024,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_feat_fusion.2.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_feat_fusion.2.weight of shape (512, 1024)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_relness_cls_fc.0.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_relness_cls_fc.0.weight of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_relness_cls_fc.2.bias of shape (50,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_relness_cls_fc.2.weight of shape (50, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.vis_embed.1.bias of shape (512,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.vis_embed.1.weight of shape (512, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.freq_bias.obj_baseline.weight of shape (22801, 51)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.freq_lambda of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.obj_classifier.bias of shape (151,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.obj_classifier.weight of shape (151, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifier.bias of shape (51,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifier.weight of shape (51, 512)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.rel_pn_thres of shape (1,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.rel_pn_thres_for_test of shape (1,)
[03/22 00:55:11 utils.model_serialization]: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                        loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
[03/22 00:55:11 utils.model_serialization]: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                      loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
[03/22 00:55:11 utils.model_serialization]: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                        loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
[03/22 00:55:11 utils.model_serialization]: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                      loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (128, 2, 7, 7)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (128,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (256,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (256, 128, 3, 3)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (256,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (256,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (256,)
[03/22 00:55:11 utils.model_serialization]: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (256,)
[03/22 00:55:11 pysgg]: #################### end distributed ####################
[03/22 00:55:11 pysgg]: Start preclser_relpn_pretrain
[03/22 00:55:11 pysgg]: Start training
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/p_zhuzy/miniconda3/envs/pysgg/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Type of images: <class 'pysgg.structures.image_list.ImageList'>
/home/p_zhuzy/miniconda3/envs/pysgg/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[03/22 00:55:23 pysgg]: ---Total norm inf clip coef 0.00000-----------------
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 8458.103516, (torch.Size([256, 1024, 3, 3]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 118.880432, (torch.Size([256]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 33689.824219, (torch.Size([4096, 12544]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 157.704010, (torch.Size([4096]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 31996.373047, (torch.Size([4096, 4096]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 349.907684, (torch.Size([4096]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 1281.300171, (torch.Size([128, 2, 7, 7]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 222.142517, (torch.Size([128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 78.600021, (torch.Size([128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 63.297550, (torch.Size([128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 4527.698730, (torch.Size([256, 128, 3, 3]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 82.545410, (torch.Size([256]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 65.427170, (torch.Size([256]))
[03/22 00:55:23 pysgg]: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 221.256271, (torch.Size([256]))
[03/22 00:55:23 pysgg]: roi_heads.relation.box_feature_extractor.fc6.weight: 576.673462, (torch.Size([4096, 12544]))
[03/22 00:55:23 pysgg]: roi_heads.relation.box_feature_extractor.fc6.bias : 3.946994, (torch.Size([4096]))
[03/22 00:55:23 pysgg]: roi_heads.relation.box_feature_extractor.fc7.weight: 697.960999, (torch.Size([4096, 4096]))
[03/22 00:55:23 pysgg]: roi_heads.relation.box_feature_extractor.fc7.bias : 14.374507, (torch.Size([4096]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_embed_on_prob_dist.weight: 2.503790, (torch.Size([151, 768]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_embed_on_pred_label.weight: 6.863647, (torch.Size([151, 768]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.rel_feature_up_dim.weight: 20232.970703, (torch.Size([2048, 4096]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.rel_feature_up_dim.bias: 1445.207397, (torch.Size([2048]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pairwise_obj_feat_updim_fc.weight: 1280.197876, (torch.Size([1024, 5376]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pairwise_obj_feat_updim_fc.bias: 66.411301, (torch.Size([1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.0.weight: 3.276002, (torch.Size([32, 9]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.0.bias: 0.000449, (torch.Size([32]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.1.weight: 0.907105, (torch.Size([32]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.1.bias: 1.434446, (torch.Size([32]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.2.weight: 6.275235, (torch.Size([128, 32]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pos_embed.2.bias: 1.843180, (torch.Size([128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.spt_emb.0.weight: 233.110870, (torch.Size([512, 32]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.spt_emb.0.bias: 72.379456, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.spt_emb.2.weight: 398.830231, (torch.Size([1024, 512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.spt_emb.2.bias: 127.941864, (torch.Size([1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pairwise_rel_feat_finalize_fc.0.weight: 517.452332, (torch.Size([2048, 1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.pairwise_rel_feat_finalize_fc.0.bias: 801.031860, (torch.Size([2048]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_hidden_linear.weight: 404.674561, (torch.Size([512, 4992]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.pairwise_feature_extractor.obj_hidden_linear.bias: 21.512215, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_sem_embed.weight: 38357.785156, (torch.Size([151, 768]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_pos_embed.0.weight: 31603.669922, (torch.Size([128, 9]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_pos_embed.0.bias: 25734.613281, (torch.Size([128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_pos_embed.2.weight: 161264.062500, (torch.Size([128, 128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.obj_pos_embed.2.bias: 67005.421875, (torch.Size([128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_box_feat_extract.1.weight: inf, (torch.Size([512, 1792]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_box_feat_extract.1.bias: inf, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.vis_embed.1.weight: 2965968.000000, (torch.Size([512, 2048]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.vis_embed.1.bias: 316432.312500, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_feat_fusion.0.weight: 67108.703125, (torch.Size([1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_feat_fusion.0.bias: 71993.031250, (torch.Size([1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_feat_fusion.2.weight: inf, (torch.Size([512, 1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_feat_fusion.2.bias: 203922.203125, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_relness_cls_fc.0.weight: 80960.765625, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_relness_cls_fc.0.bias: 82650.390625, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_relness_cls_fc.2.weight: inf, (torch.Size([50, 512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.proposal_relness_cls_fc.2.bias: 231917.593750, (torch.Size([50]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.fusion_layer.weight: inf, (torch.Size([1, 50]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.0.fusion_layer.bias: inf, (torch.Size([1]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_sem_embed.weight: 62673.796875, (torch.Size([151, 768]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_pos_embed.0.weight: 31745.572266, (torch.Size([128, 9]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_pos_embed.0.bias: 26150.613281, (torch.Size([128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_pos_embed.2.weight: 155136.593750, (torch.Size([128, 128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.obj_pos_embed.2.bias: 81996.867188, (torch.Size([128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_box_feat_extract.1.weight: inf, (torch.Size([512, 1792]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_box_feat_extract.1.bias: inf, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.vis_embed.1.weight: inf, (torch.Size([512, 512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.vis_embed.1.bias: inf, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_feat_fusion.0.weight: 131633.046875, (torch.Size([1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_feat_fusion.0.bias: 125697.250000, (torch.Size([1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_feat_fusion.2.weight: inf, (torch.Size([512, 1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_feat_fusion.2.bias: 345209.125000, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_relness_cls_fc.0.weight: 136193.312500, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_relness_cls_fc.0.bias: 138625.265625, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_relness_cls_fc.2.weight: inf, (torch.Size([50, 512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.proposal_relness_cls_fc.2.bias: inf, (torch.Size([50]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.fusion_layer.weight: inf, (torch.Size([1, 50]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.1.fusion_layer.bias: inf, (torch.Size([1]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_sem_embed.weight: 39111.156250, (torch.Size([151, 768]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_pos_embed.0.weight: 21780.761719, (torch.Size([128, 9]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_pos_embed.0.bias: 17827.453125, (torch.Size([128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_pos_embed.2.weight: 114814.257812, (torch.Size([128, 128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.obj_pos_embed.2.bias: 48044.539062, (torch.Size([128]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_box_feat_extract.1.weight: inf, (torch.Size([512, 1792]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_box_feat_extract.1.bias: inf, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.vis_embed.1.weight: 1413278.625000, (torch.Size([512, 512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.vis_embed.1.bias: inf, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_feat_fusion.0.weight: 65786.359375, (torch.Size([1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_feat_fusion.0.bias: 67759.523438, (torch.Size([1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_feat_fusion.2.weight: inf, (torch.Size([512, 1024]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_feat_fusion.2.bias: 177864.625000, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_relness_cls_fc.0.weight: 71261.023438, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_relness_cls_fc.0.bias: 72372.468750, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_relness_cls_fc.2.weight: inf, (torch.Size([50, 512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.proposal_relness_cls_fc.2.bias: 184239.937500, (torch.Size([50]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.fusion_layer.weight: inf, (torch.Size([1, 50]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.relation_conf_aware_models.2.fusion_layer.bias: inf, (torch.Size([1]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.rel_downdim_fc.0.weight: 16464.085938, (torch.Size([512, 2048]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.context_layer.rel_downdim_fc.0.bias: 1451.240479, (torch.Size([512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.rel_classifier.weight: 20512.406250, (torch.Size([51, 512]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.rel_classifier.bias  : 4516.106934, (torch.Size([51]))
[03/22 00:55:23 pysgg]: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: 407.718506, (torch.Size([22801, 51]))
[03/22 00:55:23 pysgg]: -------------------------------
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
[03/22 00:57:01 pysgg]: 
instance name: sgdet-BGNNPredictor/2025-03-22_00
elapsed time: 0:01:50
  eta: 0:16:35
  iter: 100/1000
  loss: 6.6451 (12.5707)
  loss_rel: 0.1228 (0.1133)
  pre_rel_classify_loss_iter-0: 1.9880 (3.7650)
  pre_rel_classify_loss_iter-1: 2.9628 (6.0045)
  pre_rel_classify_loss_iter-2: 1.4676 (2.6879)
  time: 0.8124 (1.1056)
  data: 0.0074 (0.0412)
  lr: 0.008960
  max mem: 19915

[03/22 00:57:01 pysgg]: relness module pretraining..
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
Type of images: <class 'pysgg.structures.image_list.ImageList'>
