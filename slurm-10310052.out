æ¨¡å‹åŠ è½½æˆåŠŸï¼
è¯æ±‡è¡¨å¤§å°: 3000000
[03/15 23:25:18 pysgg]: Using 1 GPUs
[03/15 23:25:18 pysgg]: Namespace(config_file='', local_rank=0, num_workers=4, output_dir='checkpoints/sgdet-BGNNPredictor/2025-03-15_23', dataset_name='VG_stanford', dataset_name_test='VG_stanford_test', dataset_name_val='VG_stanford_val', img_dir='datasets/vg/stanford_spilt/VG_100k_images', glove_dir='datasets/vg/stanford_spilt/glove', word2vec_dir='datasets/vg/stanford_spilt/word2vec', roidb_file='datasets/vg/VG-SGG-with-attri.h5', dict_file='datasets/vg/VG-SGG-dicts-with-attri.json', image_file='datasets/vg/image_data.json', debug=False, train_pre_batch=4, test_pre_batch=1, max_iter=1000, size_divisbility=32, base_lr=0.008, weight_decay=1e-05, bias_factor=1.0, weight_decay_bias=0.0, momentum=0.9, pretrained_detection_ckpt='checkpoints/detection/pretrained_faster_rcnn/vg_faster_det.pth', checkpoint_period=500, skip_test=False, opts=[], res_in_channels=64, res_out_channels=256, backbone_out_channels=256, box_head_num_class=151, device='cuda', distributed=False)
[03/15 23:25:18 pysgg]: #################### Start initializing dataset & dataloader ####################
[03/15 23:25:29 dataset]: using resampling method:bilvl
[03/15 23:25:29 dataset]: load repeat_dict from checkpoints/sgdet-BGNNPredictor/2025-03-15_23/repeat_dict.pkl
[03/15 23:25:29 dataset]: using resampling method:bilvl
[03/15 23:25:29 dataset]: load repeat_dict from checkpoints/sgdet-BGNNPredictor/2025-03-15_23/repeat_dict.pkl
[03/15 23:25:29 utils.miscellaneous]: Saving labels mapping into checkpoints/sgdet-BGNNPredictor/2025-03-15_23/labels.json
/home/p_zhuzy/miniconda3/envs/pysgg/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[03/15 23:25:30 pysgg]: the nums of train loader is 1000
[03/15 23:25:30 pysgg]: the nums of val loader is 5000
[03/15 23:25:30 pysgg]: #################### end dataloader ####################
[03/15 23:25:30 pysgg]: #################### prepare training ####################
ğŸ”¹ ç›´æ¥åŠ è½½ PyTorch è¯å‘é‡: datasets/vg/stanford_spilt/word2vec/GoogleNews-vectors-negative300.300d.pt
ğŸ”¹ åŠ è½½çš„è¯å‘é‡ç»´åº¦: 300
ğŸ”¹ '__background__' -> '__background__' (å°è¯•åŒ¹é…)
 æ— æ³•æ‰¾åˆ° '__background__' çš„è¯å‘é‡
Traceback (most recent call last):
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 475, in <module>
    main()
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 467, in main
    model = train(args,args.local_rank, args.distributed,logger)
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 132, in train
    model = build_detection_model(args)
  File "/project/p_zhu/PySGG-main/util/model.py", line 63, in build_detection_model
    return GeneralizedRCNN(args)
  File "/project/p_zhu/PySGG-main/util/model.py", line 25, in __init__
    self.roi_heads = build_roi_heads(args, self.backbone.out_channels)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2400, in build_roi_heads
    roi_heads.append(("relation", build_roi_relation_head(args, in_channels)))
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2333, in build_roi_relation_head
    return ROIRelationHead(args,in_channels)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2129, in __init__
    self.predictor = BGNNPredictor(args, feat_dim)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 1785, in __init__
    self.context_layer = BGNNContext(
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 1034, in __init__
    RelAwareRelFeature(args,input_dim)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 847, in __init__
    obj_embed_vecs = obj_edge_vectors(
  File "/project/p_zhu/PySGG-main/util/load_word2vec.py", line 97, in obj_edge_vectors
    wv_dict, wv_arr, wv_size = load_word_vectors(wv_dir, wv_type, wv_dim)
  File "/project/p_zhu/PySGG-main/util/load_word2vec.py", line 65, in load_word_vectors
    raise RuntimeError(f"æ‰¾ä¸åˆ° {fname_bin} æˆ– {fname_txt}")
RuntimeError: æ‰¾ä¸åˆ° datasets/vg/stanford_spilt/glove/GoogleNews-vectors-negative300.bin æˆ– datasets/vg/stanford_spilt/glove/GoogleNews-vectors-negative300.txt
