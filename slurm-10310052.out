模型加载成功！
词汇表大小: 3000000
[03/15 23:25:18 pysgg]: Using 1 GPUs
[03/15 23:25:18 pysgg]: Namespace(config_file='', local_rank=0, num_workers=4, output_dir='checkpoints/sgdet-BGNNPredictor/2025-03-15_23', dataset_name='VG_stanford', dataset_name_test='VG_stanford_test', dataset_name_val='VG_stanford_val', img_dir='datasets/vg/stanford_spilt/VG_100k_images', glove_dir='datasets/vg/stanford_spilt/glove', word2vec_dir='datasets/vg/stanford_spilt/word2vec', roidb_file='datasets/vg/VG-SGG-with-attri.h5', dict_file='datasets/vg/VG-SGG-dicts-with-attri.json', image_file='datasets/vg/image_data.json', debug=False, train_pre_batch=4, test_pre_batch=1, max_iter=1000, size_divisbility=32, base_lr=0.008, weight_decay=1e-05, bias_factor=1.0, weight_decay_bias=0.0, momentum=0.9, pretrained_detection_ckpt='checkpoints/detection/pretrained_faster_rcnn/vg_faster_det.pth', checkpoint_period=500, skip_test=False, opts=[], res_in_channels=64, res_out_channels=256, backbone_out_channels=256, box_head_num_class=151, device='cuda', distributed=False)
[03/15 23:25:18 pysgg]: #################### Start initializing dataset & dataloader ####################
[03/15 23:25:29 dataset]: using resampling method:bilvl
[03/15 23:25:29 dataset]: load repeat_dict from checkpoints/sgdet-BGNNPredictor/2025-03-15_23/repeat_dict.pkl
[03/15 23:25:29 dataset]: using resampling method:bilvl
[03/15 23:25:29 dataset]: load repeat_dict from checkpoints/sgdet-BGNNPredictor/2025-03-15_23/repeat_dict.pkl
[03/15 23:25:29 utils.miscellaneous]: Saving labels mapping into checkpoints/sgdet-BGNNPredictor/2025-03-15_23/labels.json
/home/p_zhuzy/miniconda3/envs/pysgg/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[03/15 23:25:30 pysgg]: the nums of train loader is 1000
[03/15 23:25:30 pysgg]: the nums of val loader is 5000
[03/15 23:25:30 pysgg]: #################### end dataloader ####################
[03/15 23:25:30 pysgg]: #################### prepare training ####################
🔹 直接加载 PyTorch 词向量: datasets/vg/stanford_spilt/word2vec/GoogleNews-vectors-negative300.300d.pt
🔹 加载的词向量维度: 300
🔹 '__background__' -> '__background__' (尝试匹配)
 无法找到 '__background__' 的词向量
Traceback (most recent call last):
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 475, in <module>
    main()
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 467, in main
    model = train(args,args.local_rank, args.distributed,logger)
  File "/home/p_zhuzy/p_zhu/PySGG-main/train.py", line 132, in train
    model = build_detection_model(args)
  File "/project/p_zhu/PySGG-main/util/model.py", line 63, in build_detection_model
    return GeneralizedRCNN(args)
  File "/project/p_zhu/PySGG-main/util/model.py", line 25, in __init__
    self.roi_heads = build_roi_heads(args, self.backbone.out_channels)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2400, in build_roi_heads
    roi_heads.append(("relation", build_roi_relation_head(args, in_channels)))
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2333, in build_roi_relation_head
    return ROIRelationHead(args,in_channels)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 2129, in __init__
    self.predictor = BGNNPredictor(args, feat_dim)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 1785, in __init__
    self.context_layer = BGNNContext(
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 1034, in __init__
    RelAwareRelFeature(args,input_dim)
  File "/project/p_zhu/PySGG-main/util/roi_heads.py", line 847, in __init__
    obj_embed_vecs = obj_edge_vectors(
  File "/project/p_zhu/PySGG-main/util/load_word2vec.py", line 97, in obj_edge_vectors
    wv_dict, wv_arr, wv_size = load_word_vectors(wv_dir, wv_type, wv_dim)
  File "/project/p_zhu/PySGG-main/util/load_word2vec.py", line 65, in load_word_vectors
    raise RuntimeError(f"找不到 {fname_bin} 或 {fname_txt}")
RuntimeError: 找不到 datasets/vg/stanford_spilt/glove/GoogleNews-vectors-negative300.bin 或 datasets/vg/stanford_spilt/glove/GoogleNews-vectors-negative300.txt
